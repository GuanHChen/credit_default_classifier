From the paper:
Error = 0.17
AUC = 0.536 Tree / 0.68 KNN

CREDIT CARD DEFAULT FEATURES

LIMIT_BAL: Credit Limit in NT
SEX: Gender (1=male, 2=female)
EDUCATION: (1=graduate school, 2=university, 3=high school, 4=others, 5=unknown, 6=unknown)
MARRIAGE: Marital status (1=married, 2=single, 3=others)
AGE: Age
PAY_0: Repayment status in September, 2005 (-2= no consumption, -1=pay duly, 1=payment delay for one month, 2=payment delay for two months, â€¦ 8=payment delay for eight months, 9=payment delay for nine months and above)
PAY_2: Repayment status in August, 2005 (scale same as above)
PAY_3: Repayment status in July, 2005 (scale same as above)
PAY_4: Repayment status in June, 2005 (scale same as above)
PAY_5: Repayment status in May, 2005 (scale same as above)
PAY_6: Repayment status in April, 2005 (scale same as above)
BILL_AMT1: Amount of bill statement in September, 2005 (NT dollar)
BILL_AMT2: Amount of bill statement in August, 2005 (NT dollar)
BILL_AMT3: Amount of bill statement in July, 2005 (NT dollar)
BILL_AMT4: Amount of bill statement in June, 2005 (NT dollar)
BILL_AMT5: Amount of bill statement in May, 2005 (NT dollar)
BILL_AMT6: Amount of bill statement in April, 2005 (NT dollar)
PAY_AMT1: Amount of previous payment in September, 2005 (NT dollar)
PAY_AMT2: Amount of previous payment in August, 2005 (NT dollar)
PAY_AMT3: Amount of previous payment in July, 2005 (NT dollar)
PAY_AMT4: Amount of previous payment in June, 2005 (NT dollar)
PAY_AMT5: Amount of previous payment in May, 2005 (NT dollar)
PAY_AMT6: Amount of previous payment in April, 2005 (NT dollar)
default.payment.next.month: Default payment (1=yes, 0=no)

Encoding Categorical Variables
Binary Encoding of Sex:
0 = Female     1 = Male

Ordinal Encoding of Education:
0 = Unknown      1 = High School        2 = University      3 = Graduate School

One Hot Encoding of Marriage

Step 3: Feature Engineering
Credit Util (Avg)
Late Payment Count
Bill Variance (capture spikes in spending / erratic spending behavior)

Step 4: Feature Scaling
normalize all numerical data

Step 5: Feature Selection

Step 6: Model Optimizations
Resampling
K Fold Cross Validation
Pruning
Boosting
Bagging
Random Forest / Ensemble
Credit Utilization Filter
Downsampling




Binary encoding of SEX variable
Female = 0
Male = 1


Ordinal encoding of EDUCATION
0 = Others, Unknown         280+123+51+14
1 = High School             4917
2 = University              14030
3 = Graduate School         10585



One-hot encoding of MARRIAGE
0 = Not sure what this is   54
1 = Married                 13659
2 = Single                  15964
3 = Other                   323

Single and Married will be one hot encoded
Values of 0 and for Other will be dropped. If someone is neither married or single,
the other category is implicit

Feature Engineering
Credit utilization
637 Counts have utilization greater than 1
201 Counts have utilization less than 0



Modelling
Random Forest

Normal Model:
              precision    recall  f1-score   support
           0       0.84      0.94      0.89      4673
           1       0.65      0.37      0.47      1327
    accuracy                           0.82      6000

Hyperparameter Tuned:
              precision    recall  f1-score   support
           0       0.84      0.96      0.89      4673
           1       0.70      0.36      0.48      1327
    accuracy                           0.82      6000

Optuna ROC_AUC:
              precision    recall  f1-score   support
           0       0.84      0.96      0.89      4673
           1       0.70      0.36      0.47      1327
    accuracy                           0.82      6000

Optuna F1:
              precision    recall  f1-score   support
           0       0.84      0.96      0.89      4673
           1       0.70      0.36      0.47      1327
    accuracy                           0.82      6000

Optuna Recall:
              precision    recall  f1-score   support
           0       0.84      0.96      0.89      4673
           1       0.70      0.36      0.47      1327
    accuracy                           0.82      6000

Basic XGBoost:
              precision    recall  f1-score   support
           0       0.84      0.94      0.89      4673
           1       0.62      0.37      0.46      1327
    accuracy                           0.81      6000

Hyper Hypertuned XGBoost AUCPR:
              precision    recall  f1-score   support
           0       0.84      0.96      0.89      4673
           1       0.71      0.35      0.46      1327
    accuracy                           0.82      6000

Tuned XGBoost No Regularization AUCPR:
              precision    recall  f1-score   support
           0       0.84      0.92      0.88      4673
           1       0.59      0.38      0.46      1327
    accuracy                           0.80      6000


Hyperparameter Tuning

n_estimators = trial.suggest_int('n_estimators', 100, 2000)
max_depth = trial.suggest_int('max_depth', 10, 50)
min_samples_split = trial.suggest_int('min_samples_split', 10, 1000)
min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 500)

param_grid = [{
    'n_estimators': [250, 500, 750, 1000, 1250, 1500],
    'criterion': ['entropy', 'gini'],
    'min_samples_split': [10, 100, 1000],
    'min_samples_leaf': [10, 25, 100],
    'max_depth': [10, 20, 30]
}]

grid_search = GridSearchCV(rf,
    param_grid,
    cv=5,
    scoring=['roc_auc', 'f1'],
    refit='roc_auc',
    n_jobs=-1,
    verbose=1
)

grid_search.fit(X_val, y_val)
print(grid_search.best_score_) # 0.7861319869359884
print(grid_search.best_params_) # {'criterion': 'entropy', 'max_depth': 20, 'min_samples_leaf': 10, 'min_samples_split': 100, 'n_estimators': 1500}

Basic Forest
BSL: 0.13785301673601663

Optuna ROC_AUC
{'n_estimators': 1809, 'max_depth': 31, 'min_samples_split': 123, 'min_samples_leaf': 27}
AUC: 0.7878
BSL: 0.13296771874634475

Optuna F1
{'n_estimators': 1086, 'max_depth': 41, 'min_samples_split': 115, 'min_samples_leaf': 2}
AUC: 0.7874
BSL: 0.13280654330297106

Basic XGBoost: 1000 estimators, 0.1 learning rate, max depth 50
AUC: 0.7509
BSL: 0.1635929576630231

Hyper Hypertuned XGBoost
{'n_estimators': 1359, 'eta': 0.09796148646799241, 'max_depth': 32, 'gamma': 9, 'alpha': 8, 'reg_lambda': 6, 'max_delta_step': 1, 'min_child_weight': 16}
AUC: 0.7872
BSL: 0.1329201050971384

Tuned XGBoost No Regularization AUCPR:
AUC: 0.7446
BSL: 0.15673727567788756

Random Forest Robustness No Repay1
late       0.235702
repay2     0.125866
AUC Score: 0.7797
Prec .64
Rec 0.28
BSL 0.13846998941423283

Random Forest Robustness No Late
repay1     0.256655
repay2     0.117113
AUC Score: 0.7870
Prec .70
Rec .35
BSL 0.1332414646747992


